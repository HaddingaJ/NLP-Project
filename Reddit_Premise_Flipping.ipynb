{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a12ecdad33247de998bc6081e37a626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_debffa8244234eaa84aec81553669500",
              "IPY_MODEL_f1e3cdaa1b814f69b8673f151c53c54e",
              "IPY_MODEL_e732a75ca61b492e9ec08a173d6a2773"
            ],
            "layout": "IPY_MODEL_8364d88d3cdc4291ba0e149a68179aa1"
          }
        },
        "debffa8244234eaa84aec81553669500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b945957413db4e12868af8cd89ac5a0d",
            "placeholder": "​",
            "style": "IPY_MODEL_ec7097725975416298172d8ffb3e751d",
            "value": "NLI orig: 100%"
          }
        },
        "f1e3cdaa1b814f69b8673f151c53c54e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ad2233679b4a169b5a33a6b66453c8",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_309386c1a1aa495db0b8c975d13995b6",
            "value": 25
          }
        },
        "e732a75ca61b492e9ec08a173d6a2773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86e19db950a94ed5960505a48a26a694",
            "placeholder": "​",
            "style": "IPY_MODEL_6c253eb11a0e4051b14c211d9afaaf77",
            "value": " 25/25 [11:01&lt;00:00, 25.50s/it]"
          }
        },
        "8364d88d3cdc4291ba0e149a68179aa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b945957413db4e12868af8cd89ac5a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec7097725975416298172d8ffb3e751d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7ad2233679b4a169b5a33a6b66453c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309386c1a1aa495db0b8c975d13995b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86e19db950a94ed5960505a48a26a694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c253eb11a0e4051b14c211d9afaaf77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6969bac51578466cb5c4e6602cc2d68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f038ec97c8f94213bf6609b2ad8c5a63",
              "IPY_MODEL_9b4bcd8158434f58bab13d7c8c8719a2",
              "IPY_MODEL_85efcc2c9fbc4837b8a623bea65cb829"
            ],
            "layout": "IPY_MODEL_de99e10b32e44680b98fc1f8d07364a6"
          }
        },
        "f038ec97c8f94213bf6609b2ad8c5a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_310ac3b1861b4f939c5033979ccbbf63",
            "placeholder": "​",
            "style": "IPY_MODEL_70c750aaa71f4632a5bb10d95421fc94",
            "value": "NLI flip: 100%"
          }
        },
        "9b4bcd8158434f58bab13d7c8c8719a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f971eadcac894e4ca9d85b0868ac2370",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55487476c3c64054ac8fa300becfc527",
            "value": 25
          }
        },
        "85efcc2c9fbc4837b8a623bea65cb829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_572560788b2b40abaf8c811eb82b783c",
            "placeholder": "​",
            "style": "IPY_MODEL_ed9a9efd74bb4b01919aa4ffa7218103",
            "value": " 25/25 [10:50&lt;00:00, 25.07s/it]"
          }
        },
        "de99e10b32e44680b98fc1f8d07364a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "310ac3b1861b4f939c5033979ccbbf63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c750aaa71f4632a5bb10d95421fc94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f971eadcac894e4ca9d85b0868ac2370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55487476c3c64054ac8fa300becfc527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "572560788b2b40abaf8c811eb82b783c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9a9efd74bb4b01919aa4ffa7218103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8dNG5BPggWX",
        "outputId": "e5af6e65-8ebd-4e33-f6c0-6bdf184b46b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall \"torch==2.3.0\" \"torchvision\" \"torchaudio\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E8KsdrPmg03R",
        "outputId": "e6dcb9a6-86dc-4c8c-9589-e30b318a87fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting filelock (from torch==2.3.0)\n",
            "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.3.0)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy (from torch==2.3.0)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.3.0)\n",
            "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jinja2 (from torch==2.3.0)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.3.0)\n",
            "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting numpy (from torchvision)\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Downloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Downloading torchvision-0.18.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
            "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading torchaudio-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Downloading torchaudio-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "INFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torchaudio-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Downloading torchaudio-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Downloading torchaudio-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Downloading torchaudio-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.3.0)\n",
            "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.3.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.6.1\n",
            "    Uninstalling networkx-3.6.1:\n",
            "      Successfully uninstalled networkx-3.6.1\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.20.0\n",
            "    Uninstalling filelock-3.20.0:\n",
            "      Successfully uninstalled filelock-3.20.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cu126\n",
            "    Uninstalling torchaudio-2.9.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.12.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.12.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.12.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6.1 numpy-2.3.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-12.0.0 sympy-1.14.0 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy",
                  "torchgen"
                ]
              },
              "id": "c7911bc7b69f49fdbc09842b6b1c1ec2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "integrated_mnli_inference_flipped_subset.py\n",
        "\n",
        "Run MNLI-based NLI with BART and RoBERTa on a *small subset* of Reddit\n",
        "propositions, in BOTH directions:\n",
        "\n",
        "  (1) Original: premise = proposition, hypothesis = fixed agency sentence.\n",
        "  (2) Flipped:  premise = fixed agency sentence, hypothesis = proposition.\n",
        "\n",
        "Export:\n",
        "\n",
        "- Per-model NLI probabilities in both directions:\n",
        "    * P(ENTAILMENT), P(NEUTRAL), P(CONTRADICTION)\n",
        "- Per-model agency scores in both directions:\n",
        "    * s_m(p) = P_m(ENT) - P_m(CON)\n",
        "- Mean agency scores (original vs flipped).\n",
        "\n",
        "NOTE:\n",
        "No ±0.3 thresholding or winner-takes-strongest here; this script is just for\n",
        "sanity-checking / probing the effect of premise–hypothesis flipping.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 1. Configuration\n",
        "\n",
        "MODELS = {\n",
        "    \"BART_MNLI\": \"facebook/bart-large-mnli\",\n",
        "    \"ROBERTA_MNLI\": \"roberta-large-mnli\",\n",
        "}\n",
        "\n",
        "FIXED_HYPOTHESIS_AGENCY = (\n",
        "    \"The proposition refers to the ability of humans to make choices, \"\n",
        "    \"exert control, or take responsibility for the actions and outcomes of AI.\"\n",
        ")\n",
        "\n",
        "DATA_FILENAME   = \"/content/drive/MyDrive/NLP /artificial_filtered_output.jsonl\"\n",
        "OUTPUT_FILENAME = \"ai_human_agency_mnli_subset_flipped.csv\"\n",
        "PROPOSITION_COLUMN = \"proposition\"\n",
        "\n",
        "# How many propositions to sample for the flip experiment\n",
        "SUBSET_N = 100  # tweak as you like\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2. Data loading (and subsetting)\n",
        "\n",
        "if not os.path.exists(DATA_FILENAME):\n",
        "    raise FileNotFoundError(f\"Input file not found: {DATA_FILENAME}\")\n",
        "\n",
        "df_full = pd.read_json(DATA_FILENAME, lines=True)\n",
        "\n",
        "if PROPOSITION_COLUMN not in df_full.columns:\n",
        "    raise KeyError(\n",
        "        f\"Column '{PROPOSITION_COLUMN}' not found. \"\n",
        "        f\"Available columns: {list(df_full.columns)}\"\n",
        "    )\n",
        "\n",
        "df_full[PROPOSITION_COLUMN] = df_full[PROPOSITION_COLUMN].fillna(\"\").astype(str)\n",
        "\n",
        "if SUBSET_N is not None and SUBSET_N < len(df_full):\n",
        "    df = df_full.sample(n=SUBSET_N, random_state=42).reset_index(drop=True)\n",
        "    print(f\"Sampled subset of size {len(df)} from {len(df_full)} total rows.\")\n",
        "else:\n",
        "    df = df_full.copy().reset_index(drop=True)\n",
        "    print(f\"Using full dataset of size {len(df)} (no subsetting).\")\n",
        "\n",
        "sentences = df[PROPOSITION_COLUMN].tolist()\n",
        "\n",
        "# 3. General MNLI runner for arbitrary premise/hypothesis pairing\n",
        "\n",
        "def run_mnli_pairs(model_id, premises, hypotheses, batch_size=None, suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Run MNLI with arbitrary (premise, hypothesis) pairs.\n",
        "\n",
        "    Returns a DataFrame with columns:\n",
        "\n",
        "        {model_short}_entailment{suffix}\n",
        "        {model_short}_neutral{suffix}\n",
        "        {model_short}_contradiction{suffix}\n",
        "    \"\"\"\n",
        "    assert len(premises) == len(hypotheses), \"Premises and hypotheses must align.\"\n",
        "\n",
        "    print(f\"\\n--- Running MNLI NLI with model: {model_id} ({suffix or 'orig'}) ---\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = 32 if device.type == \"cuda\" else 8\n",
        "\n",
        "    ent_scores = []\n",
        "    neu_scores = []\n",
        "    con_scores = []\n",
        "\n",
        "    # Normalize labels to uppercase for robust mapping\n",
        "    id2label = {i: lbl.upper() for i, lbl in model.config.id2label.items()}\n",
        "    print(f\"Model label mapping: {id2label}\")\n",
        "\n",
        "    for i in tqdm(range(0, len(premises), batch_size),\n",
        "                  desc=f\"NLI {model_id.split('/')[-1]}{suffix}\"):\n",
        "        prem_batch = premises[i:i + batch_size]\n",
        "        hyp_batch  = hypotheses[i:i + batch_size]\n",
        "\n",
        "        enc = tokenizer(\n",
        "            prem_batch,\n",
        "            hyp_batch,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits\n",
        "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "        for row in probs:\n",
        "            scores = {id2label[j]: float(row[j]) for j in range(len(row))}\n",
        "            ent_scores.append(scores.get(\"ENTAILMENT\", 0.0))\n",
        "            neu_scores.append(scores.get(\"NEUTRAL\", 0.0))\n",
        "            con_scores.append(scores.get(\"CONTRADICTION\", 0.0))\n",
        "\n",
        "    model_short = model_id.split(\"/\")[-1]\n",
        "    return pd.DataFrame({\n",
        "        f\"{model_short}_entailment{suffix}\": ent_scores,\n",
        "        f\"{model_short}_neutral{suffix}\": neu_scores,\n",
        "        f\"{model_short}_contradiction{suffix}\": con_scores,\n",
        "    })\n",
        "\n",
        "# 4. Run both models: original vs flipped\n",
        "\n",
        "df_results = df.copy()\n",
        "\n",
        "prem_orig = sentences\n",
        "hypo_orig = [FIXED_HYPOTHESIS_AGENCY] * len(sentences)\n",
        "\n",
        "prem_flip = [FIXED_HYPOTHESIS_AGENCY] * len(sentences)\n",
        "hypo_flip = sentences\n",
        "\n",
        "for _, model_id in MODELS.items():\n",
        "    # Original direction\n",
        "    df_orig = run_mnli_pairs(\n",
        "        model_id,\n",
        "        premises=prem_orig,\n",
        "        hypotheses=hypo_orig,\n",
        "        suffix=\"_orig\"\n",
        "    )\n",
        "    # Flipped direction\n",
        "    df_flip = run_mnli_pairs(\n",
        "        model_id,\n",
        "        premises=prem_flip,\n",
        "        hypotheses=hypo_flip,\n",
        "        suffix=\"_flip\"\n",
        "    )\n",
        "\n",
        "    df_results = pd.concat([df_results, df_orig, df_flip], axis=1)\n",
        "\n",
        "# 5. Compute agency scores (orig vs flip) and mean scores\n",
        "\n",
        "def safe_col(df_, name: str) -> pd.Series:\n",
        "    \"\"\"Return a float Series for the given column name, or error if missing.\"\"\"\n",
        "    if name not in df_.columns:\n",
        "        raise KeyError(f\"Expected column missing: {name}\")\n",
        "    return df_[name].astype(float)\n",
        "\n",
        "# BART\n",
        "bart_ent_orig = safe_col(df_results, \"bart-large-mnli_entailment_orig\")\n",
        "bart_con_orig = safe_col(df_results, \"bart-large-mnli_contradiction_orig\")\n",
        "bart_ent_flip = safe_col(df_results, \"bart-large-mnli_entailment_flip\")\n",
        "bart_con_flip = safe_col(df_results, \"bart-large-mnli_contradiction_flip\")\n",
        "\n",
        "df_results[\"bart-large-mnli_agency_score_orig\"] = bart_ent_orig - bart_con_orig\n",
        "df_results[\"bart-large-mnli_agency_score_flip\"] = bart_ent_flip - bart_con_flip\n",
        "\n",
        "# RoBERTa\n",
        "rob_ent_orig = safe_col(df_results, \"roberta-large-mnli_entailment_orig\")\n",
        "rob_con_orig = safe_col(df_results, \"roberta-large-mnli_contradiction_orig\")\n",
        "rob_ent_flip = safe_col(df_results, \"roberta-large-mnli_entailment_flip\")\n",
        "rob_con_flip = safe_col(df_results, \"roberta-large-mnli_contradiction_flip\")\n",
        "\n",
        "df_results[\"roberta-large-mnli_agency_score_orig\"] = rob_ent_orig - rob_con_orig\n",
        "df_results[\"roberta-large-mnli_agency_score_flip\"] = rob_ent_flip - rob_con_flip\n",
        "\n",
        "# Mean agency scores\n",
        "df_results[\"mean_agency_score_orig\"] = (\n",
        "    df_results[\"bart-large-mnli_agency_score_orig\"] +\n",
        "    df_results[\"roberta-large-mnli_agency_score_orig\"]\n",
        ") / 2.0\n",
        "\n",
        "df_results[\"mean_agency_score_flip\"] = (\n",
        "    df_results[\"bart-large-mnli_agency_score_flip\"] +\n",
        "    df_results[\"roberta-large-mnli_agency_score_flip\"]\n",
        ") / 2.0\n",
        "\n",
        "# 6. Export a compact comparison CSV\n",
        "\n",
        "output_columns = [\n",
        "    PROPOSITION_COLUMN,\n",
        "    # BART original / flipped\n",
        "    \"bart-large-mnli_entailment_orig\",\n",
        "    \"bart-large-mnli_contradiction_orig\",\n",
        "    \"bart-large-mnli_agency_score_orig\",\n",
        "    \"bart-large-mnli_entailment_flip\",\n",
        "    \"bart-large-mnli_contradiction_flip\",\n",
        "    \"bart-large-mnli_agency_score_flip\",\n",
        "    # RoBERTa original / flipped\n",
        "    \"roberta-large-mnli_entailment_orig\",\n",
        "    \"roberta-large-mnli_contradiction_orig\",\n",
        "    \"roberta-large-mnli_agency_score_orig\",\n",
        "    \"roberta-large-mnli_entailment_flip\",\n",
        "    \"roberta-large-mnli_contradiction_flip\",\n",
        "    \"roberta-large-mnli_agency_score_flip\",\n",
        "    # Mean agency\n",
        "    \"mean_agency_score_orig\",\n",
        "    \"mean_agency_score_flip\",\n",
        "]\n",
        "\n",
        "existing_output_columns = [c for c in output_columns if c in df_results.columns]\n",
        "df_output = df_results[existing_output_columns].copy()\n",
        "\n",
        "df_output.to_csv(OUTPUT_FILENAME, index=False)\n",
        "\n",
        "print(\"\\n--- MNLI Flip Experiment Complete ---\")\n",
        "print(f\"Results saved to: {OUTPUT_FILENAME}\")\n",
        "print(df_output.head())\n",
        "print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "6B11yrdwxAQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "mnli_flip_sanity_check.py\n",
        "\n",
        "Goal:\n",
        "- Take a small subset of your Reddit propositions.\n",
        "- Run MNLI NLI with BART in two directions:\n",
        "    1) ORIGINAL:   premise = proposition, hypothesis = FIXED_HYPOTHESIS_AGENCY\n",
        "    2) FLIPPED:    premise = FIXED_HYPOTHESIS_AGENCY, hypothesis = proposition\n",
        "- For each direction, extract:\n",
        "    P(ENTAILMENT), P(NEUTRAL), P(CONTRADICTION)\n",
        "    agency_score = P(ENTAILMENT) - P(CONTRADICTION)\n",
        "- Save a comparison CSV for manual inspection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 1. Config\n",
        "\n",
        "MODEL_ID = \"facebook/bart-large-mnli\"\n",
        "\n",
        "DATA_FILENAME   = \"/content/drive/MyDrive/NLP /artificial_filtered_output.jsonl\"\n",
        "OUTPUT_FILENAME = \"mnli_flip_compare_sample.csv\"\n",
        "PROPOSITION_COLUMN = \"proposition\"\n",
        "\n",
        "# How many propositions to test\n",
        "N_SAMPLE = 200  # change if you want more/less\n",
        "\n",
        "FIXED_HYPOTHESIS_AGENCY = (\n",
        "    \"The proposition refers to the ability of humans to make choices, \"\n",
        "    \"exert control, or take responsibility for the actions and outcomes of AI.\"\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2. Load data and pick subset\n",
        "\n",
        "if not os.path.exists(DATA_FILENAME):\n",
        "    raise FileNotFoundError(f\"Input file not found: {DATA_FILENAME}\")\n",
        "\n",
        "df = pd.read_json(DATA_FILENAME, lines=True)\n",
        "\n",
        "if PROPOSITION_COLUMN not in df.columns:\n",
        "    raise KeyError(\n",
        "        f\"Column '{PROPOSITION_COLUMN}' not found. \"\n",
        "        f\"Available columns: {list(df.columns)}\"\n",
        "    )\n",
        "\n",
        "df[PROPOSITION_COLUMN] = df[PROPOSITION_COLUMN].fillna(\"\").astype(str)\n",
        "\n",
        "# Deterministic subset: first N rows (you can use .sample(N_SAMPLE, random_state=42) instead)\n",
        "df_subset = df.head(N_SAMPLE).copy().reset_index(drop=True)\n",
        "sentences = df_subset[PROPOSITION_COLUMN].tolist()\n",
        "\n",
        "print(f\"Loaded {len(df_subset)} propositions for flip sanity check.\")\n",
        "\n",
        "# 3. Helper: run MNLI NLI for arbitrary (premise, hypothesis) pairing\n",
        "\n",
        "def run_mnli_nli_direction(model_id, premises, hypotheses, suffix, batch_size=None):\n",
        "    \"\"\"\n",
        "    Run MNLI NLI with:\n",
        "        premise   = premises[i]\n",
        "        hypothesis = hypotheses[i]\n",
        "\n",
        "    Returns a DataFrame with columns:\n",
        "        bart-large-mnli_entailment_{suffix}\n",
        "        bart-large-mnli_neutral_{suffix}\n",
        "        bart-large-mnli_contradiction_{suffix}\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running MNLI with {model_id} [{suffix}] ---\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = 32 if device.type == \"cuda\" else 8\n",
        "\n",
        "    ent_scores = []\n",
        "    neu_scores = []\n",
        "    con_scores = []\n",
        "\n",
        "    # Normalize labels to uppercase for robust mapping\n",
        "    id2label = {i: lbl.upper() for i, lbl in model.config.id2label.items()}\n",
        "    print(f\"Label mapping: {id2label}\")\n",
        "\n",
        "    for i in tqdm(range(0, len(premises), batch_size),\n",
        "                  desc=f\"NLI {suffix}\"):\n",
        "        prem_batch = premises[i:i + batch_size]\n",
        "        hyp_batch  = hypotheses[i:i + batch_size]\n",
        "\n",
        "        enc = tokenizer(\n",
        "            prem_batch,\n",
        "            hyp_batch,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits\n",
        "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "        for row in probs:\n",
        "            scores = {id2label[j]: row[j] for j in range(len(row))}\n",
        "            ent_scores.append(scores.get(\"ENTAILMENT\", 0.0))\n",
        "            neu_scores.append(scores.get(\"NEUTRAL\", 0.0))\n",
        "            con_scores.append(scores.get(\"CONTRADICTION\", 0.0))\n",
        "\n",
        "    model_short = model_id.split(\"/\")[-1]\n",
        "    out_df = pd.DataFrame({\n",
        "        f\"{model_short}_entailment_{suffix}\": ent_scores,\n",
        "        f\"{model_short}_neutral_{suffix}\": neu_scores,\n",
        "        f\"{model_short}_contradiction_{suffix}\": con_scores,\n",
        "    })\n",
        "\n",
        "    return out_df\n",
        "\n",
        "\n",
        "# 4. Run ORIGINAL and FLIPPED directions\n",
        "\n",
        "# ORIGINAL: premise = proposition, hypothesis = fixed hypothesis\n",
        "orig_premises   = sentences\n",
        "orig_hypotheses = [FIXED_HYPOTHESIS_AGENCY] * len(sentences)\n",
        "\n",
        "df_orig = run_mnli_nli_direction(\n",
        "    MODEL_ID,\n",
        "    orig_premises,\n",
        "    orig_hypotheses,\n",
        "    suffix=\"orig\"\n",
        ")\n",
        "\n",
        "# FLIPPED: premise = fixed hypothesis, hypothesis = proposition\n",
        "flip_premises   = [FIXED_HYPOTHESIS_AGENCY] * len(sentences)\n",
        "flip_hypotheses = sentences\n",
        "\n",
        "df_flip = run_mnli_nli_direction(\n",
        "    MODEL_ID,\n",
        "    flip_premises,\n",
        "    flip_hypotheses,\n",
        "    suffix=\"flip\"\n",
        ")\n",
        "\n",
        "# Merge into subset DataFrame\n",
        "df_results = pd.concat([df_subset, df_orig, df_flip], axis=1)\n",
        "\n",
        "# 5. Compute agency scores for orig vs flip\n",
        "\n",
        "# Extract as floats\n",
        "bart_ent_orig = df_results[\"bart-large-mnli_entailment_orig\"].astype(float)\n",
        "bart_con_orig = df_results[\"bart-large-mnli_contradiction_orig\"].astype(float)\n",
        "bart_ent_flip = df_results[\"bart-large-mnli_entailment_flip\"].astype(float)\n",
        "bart_con_flip = df_results[\"bart-large-mnli_contradiction_flip\"].astype(float)\n",
        "\n",
        "df_results[\"bart-large-mnli_agency_score_orig\"] = bart_ent_orig - bart_con_orig\n",
        "df_results[\"bart-large-mnli_agency_score_flip\"] = bart_ent_flip - bart_con_flip\n",
        "\n",
        "# Optional: quick delta to eyeball asymmetry\n",
        "df_results[\"bart-large-mnli_agency_score_delta\"] = (\n",
        "    df_results[\"bart-large-mnli_agency_score_orig\"] -\n",
        "    df_results[\"bart-large-mnli_agency_score_flip\"]\n",
        ")\n",
        "\n",
        "\n",
        "# 6. Save comparison CSV\n",
        "\n",
        "cols_for_export = [\n",
        "    PROPOSITION_COLUMN,\n",
        "    \"bart-large-mnli_entailment_orig\",\n",
        "    \"bart-large-mnli_contradiction_orig\",\n",
        "    \"bart-large-mnli_agency_score_orig\",\n",
        "    \"bart-large-mnli_entailment_flip\",\n",
        "    \"bart-large-mnli_contradiction_flip\",\n",
        "    \"bart-large-mnli_agency_score_flip\",\n",
        "    \"bart-large-mnli_agency_score_delta\",\n",
        "]\n",
        "\n",
        "df_results[cols_for_export].to_csv(OUTPUT_FILENAME, index=False)\n",
        "print(f\"\\nWrote flip comparison file: {OUTPUT_FILENAME}\")\n",
        "\n",
        "# Optional: show top rows with biggest disagreement between orig and flip\n",
        "print(\"\\nTop 10 propositions by |orig - flip| in agency score:\\n\")\n",
        "print(\n",
        "    df_results.sort_values(\"bart-large-mnli_agency_score_delta\", key=lambda s: s.abs(), ascending=False)\n",
        "    [[PROPOSITION_COLUMN,\n",
        "      \"bart-large-mnli_agency_score_orig\",\n",
        "      \"bart-large-mnli_agency_score_flip\",\n",
        "      \"bart-large-mnli_agency_score_delta\"]]\n",
        "    .head(10)\n",
        "    .to_string(index=False)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530,
          "referenced_widgets": [
            "2a12ecdad33247de998bc6081e37a626",
            "debffa8244234eaa84aec81553669500",
            "f1e3cdaa1b814f69b8673f151c53c54e",
            "e732a75ca61b492e9ec08a173d6a2773",
            "8364d88d3cdc4291ba0e149a68179aa1",
            "b945957413db4e12868af8cd89ac5a0d",
            "ec7097725975416298172d8ffb3e751d",
            "e7ad2233679b4a169b5a33a6b66453c8",
            "309386c1a1aa495db0b8c975d13995b6",
            "86e19db950a94ed5960505a48a26a694",
            "6c253eb11a0e4051b14c211d9afaaf77",
            "6969bac51578466cb5c4e6602cc2d68a",
            "f038ec97c8f94213bf6609b2ad8c5a63",
            "9b4bcd8158434f58bab13d7c8c8719a2",
            "85efcc2c9fbc4837b8a623bea65cb829",
            "de99e10b32e44680b98fc1f8d07364a6",
            "310ac3b1861b4f939c5033979ccbbf63",
            "70c750aaa71f4632a5bb10d95421fc94",
            "f971eadcac894e4ca9d85b0868ac2370",
            "55487476c3c64054ac8fa300becfc527",
            "572560788b2b40abaf8c811eb82b783c",
            "ed9a9efd74bb4b01919aa4ffa7218103"
          ]
        },
        "id": "qqheI0U9pZQu",
        "outputId": "3305c3a0-d077-43d8-95e7-ff8e242d5ad2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loaded 200 propositions for flip sanity check.\n",
            "\n",
            "--- Running MNLI with facebook/bart-large-mnli [orig] ---\n",
            "Label mapping: {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "NLI orig:   0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a12ecdad33247de998bc6081e37a626"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running MNLI with facebook/bart-large-mnli [flip] ---\n",
            "Label mapping: {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "NLI flip:   0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6969bac51578466cb5c4e6602cc2d68a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Wrote flip comparison file: mnli_flip_compare_sample.csv\n",
            "\n",
            "Top 10 propositions by |orig - flip| in agency score:\n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 proposition  bart-large-mnli_agency_score_orig  bart-large-mnli_agency_score_flip  bart-large-mnli_agency_score_delta\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          I'd like to see an AI that seeds social media with noise, disrupting any other AI that gathers my posts and analyzes them for targeted advertising                           0.618788                          -0.897140                            1.515928\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             I don't support AI music, sorry                           0.547417                          -0.957324                            1.504742\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Do you trust this human ape; Because its brain is going to be amplified by AGI long before it is enslaved to it - as short as that timeframe may be                           0.460127                          -0.947841                            1.407968\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Googles AI just laughed at me                           0.415544                          -0.942915                            1.358459\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Those cultures are free to develop their own AI                           0.722605                          -0.426483                            1.149088\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Did someone, literally anybody on earth, think AI was a fad                           0.321849                          -0.799111                            1.120960\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       AI art, is not art; Only humans can make art worthy of your attention                           0.861404                          -0.174544                            1.035948\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           a language model that tokenizes text and predicts patterns is definitely sentient                           0.189529                          -0.833253                            1.022783\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          neuralink can't read your thoughts                           0.094759                          -0.927284                            1.022043\n",
            "The term \"artificial intelligence\" was coined in 1956 by John McCarthy in his seminal paper on the subject; The \"classic literal definition\" by the guy that coined the term is simply \"the science and engineering of; If we presume we mean \"human-scale\" intelligence, does that not ignore, say, a honeybee, which is clearly intelligent in its own right, let alone the collective intelligence of the hive as its own organism (a single bee cannot swarm; a hive does; Another important distinction with respect to \"what is intelligence\" is what *isn't* intelligence; In the field of AI linear regression isn't considered part of the domain, rather, it's considered; are statistical in nature, too, just *non-linear* statistics; As such, it's suggested that at least one criterion for intelligence (artificial or otherwise) is that it is non-linear in nature; However, that, in itself, could be argued: why *can't* a linear regression, for instance, be considered; 's no non-arbitrary reason why not, in large part because we don't have a rock-solid definition for; It's also important not to conflate similar terms; Machine learning\" is not *necessarily* \"artificial intelligence\" as, with the example of linear regression above, machine learning is often the product of vastly parallel, but ultimately simple linear regression resulting in an optimized model to perform some function or offer a particular output; That said, the lines are certainly blurred                          -0.972154                           0.029465                           -1.001618\n"
          ]
        }
      ]
    }
  ]
}