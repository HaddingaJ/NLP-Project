{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pBD1J1LzgjGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4cDIL_pesi-"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "integrated_mnli_inference.py\n",
        "\n",
        "Run MNLI-based NLI with BART and RoBERTa on Reddit propositions and export:\n",
        "\n",
        "- Per-model NLI probabilities:\n",
        "    * P(ENTAILMENT), P(NEUTRAL), P(CONTRADICTION)\n",
        "- Per-model agency scores:\n",
        "    * s_m(p) = P_m(ENTAILMENT | p, h) - P_m(CONTRADICTION | p, h)\n",
        "- Combined mean agency score across models:\n",
        "    * s_mean(p) = (s_BART(p) + s_RoBERTa(p)) / 2\n",
        "- (Optional) legacy positive-consensus flag using entailment-only scores.\n",
        "\n",
        "NOTE:\n",
        "This script *does not* perform the ±0.3 POS/NEG/NON thresholding or winner-\n",
        "takes-strongest ensemble; that logic is applied downstream in a separate script.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 1. Configuration\n",
        "\n",
        "MODELS = {\n",
        "    \"BART_MNLI\": \"facebook/bart-large-mnli\",\n",
        "    \"ROBERTA_MNLI\": \"roberta-large-mnli\",\n",
        "}\n",
        "\n",
        "# Fixed hypothesis for human agency wrt AI (keep exactly as in your pipeline)\n",
        "FIXED_HYPOTHESIS_AGENCY = (\n",
        "    \"The proposition refers to the ability of humans to make choices, \"\n",
        "    \"exert control, or take responsibility for the actions and outcomes of AI.\"\n",
        ")\n",
        "\n",
        "DATA_FILENAME   = \"/content/drive/MyDrive/NLP /artificial_filtered_output.jsonl\"\n",
        "OUTPUT_FILENAME = \"ai_human_agency_inferences_mnli.csv\"\n",
        "PROPOSITION_COLUMN = \"proposition\"\n",
        "\n",
        "# Legacy consensus-entailment threshold (from the old zero-shot script)\n",
        "CONSENSUS_ENTAIL_THRESHOLD = 0.4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# 2. Data loading\n",
        "\n",
        "if not os.path.exists(DATA_FILENAME):\n",
        "    raise FileNotFoundError(f\"Input file not found: {DATA_FILENAME}\")\n",
        "\n",
        "df = pd.read_json(DATA_FILENAME, lines=True)\n",
        "\n",
        "if PROPOSITION_COLUMN not in df.columns:\n",
        "    raise KeyError(\n",
        "        f\"Column '{PROPOSITION_COLUMN}' not found. \"\n",
        "        f\"Available columns: {list(df.columns)}\"\n",
        "    )\n",
        "\n",
        "df[PROPOSITION_COLUMN] = df[PROPOSITION_COLUMN].fillna(\"\").astype(str)\n",
        "sentences = df[PROPOSITION_COLUMN].tolist()\n",
        "\n",
        "print(f\"Loaded {len(sentences)} propositions from {DATA_FILENAME}.\")\n",
        "\n",
        "# 3. Proper MNLI NLI runner\n",
        "\n",
        "def run_mnli_nli(model_id, input_sentences, fixed_hypothesis, batch_size=None):\n",
        "    \"\"\"\n",
        "    Run true MNLI-style NLI:\n",
        "\n",
        "        premise   = proposition\n",
        "        hypothesis = fixed_hypothesis\n",
        "\n",
        "    Returns a DataFrame with columns:\n",
        "\n",
        "        {model_short}_entailment\n",
        "        {model_short}_neutral\n",
        "        {model_short}_contradiction\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running MNLI NLI with model: {model_id} ---\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = 32 if device.type == \"cuda\" else 8\n",
        "\n",
        "    ent_scores = []\n",
        "    neu_scores = []\n",
        "    con_scores = []\n",
        "\n",
        "    # Normalize labels to uppercase for robust mapping\n",
        "    id2label = {i: lbl.upper() for i, lbl in model.config.id2label.items()}\n",
        "    print(f\"Model label mapping: {id2label}\")\n",
        "\n",
        "    for i in tqdm(range(0, len(input_sentences), batch_size),\n",
        "                  desc=f\"NLI {model_id.split('/')[-1]}\"):\n",
        "        batch = input_sentences[i:i + batch_size]\n",
        "\n",
        "        enc = tokenizer(\n",
        "            batch,\n",
        "            [fixed_hypothesis] * len(batch),\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits\n",
        "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "        for row in probs:\n",
        "            scores = {id2label[j]: row[j] for j in range(len(row))}\n",
        "            ent_scores.append(scores.get(\"ENTAILMENT\", 0.0))\n",
        "            neu_scores.append(scores.get(\"NEUTRAL\", 0.0))\n",
        "            con_scores.append(scores.get(\"CONTRADICTION\", 0.0))\n",
        "\n",
        "    model_short = model_id.split(\"/\")[-1]\n",
        "    out_df = pd.DataFrame({\n",
        "        f\"{model_short}_entailment\": ent_scores,\n",
        "        f\"{model_short}_neutral\": neu_scores,\n",
        "        f\"{model_short}_contradiction\": con_scores,\n",
        "    })\n",
        "\n",
        "    return out_df\n",
        "\n",
        "# 4. Run NLI for both models and merge\n",
        "\n",
        "df_results = df.copy()\n",
        "\n",
        "for _, model_id in MODELS.items():\n",
        "    model_df = run_mnli_nli(model_id, sentences, FIXED_HYPOTHESIS_AGENCY)\n",
        "    df_results = pd.concat([df_results, model_df], axis=1)\n",
        "\n",
        "# 5. Compute agency scores and mean score\n",
        "\n",
        "def safe_col(df_, name: str) -> str:\n",
        "    if name not in df_.columns:\n",
        "        raise KeyError(f\"Expected column missing: {name}\")\n",
        "    return name\n",
        "\n",
        "bart_ent = safe_col(df_results, \"bart-large-mnli_entailment\")\n",
        "bart_con = safe_col(df_results, \"bart-large-mnli_contradiction\")\n",
        "rob_ent  = safe_col(df_results, \"roberta-large-mnli_entailment\")\n",
        "rob_con  = safe_col(df_results, \"roberta-large-mnli_contradiction\")\n",
        "\n",
        "# Per-model agency scores: s_m = P(ENT) - P(CON)\n",
        "df_results[\"bart-large-mnli_agency_score\"] = df_results[bart_ent] - df_results[bart_con]\n",
        "df_results[\"roberta-large-mnli_agency_score\"] = df_results[rob_ent] - df_results[rob_con]\n",
        "\n",
        "# Mean agency score across the two models (your s_mean)\n",
        "df_results[\"mean_agency_score\"] = (\n",
        "    df_results[\"bart-large-mnli_agency_score\"] +\n",
        "    df_results[\"roberta-large-mnli_agency_score\"]\n",
        ") / 2.0\n",
        "\n",
        "# 6. Legacy “consensus entailment” flag (optional diagnostic)\n",
        "\n",
        "# This reproduces the spirit of the first script:\n",
        "#   both models have high entailment-only scores → \"highly likely agency\".\n",
        "# It is *not* the main SoA label for the paper; the ±0.3 and winner-takes-strongest\n",
        "# logic is applied in a downstream script.\n",
        "df_results[\"consensus_entail_pos_flag\"] = (\n",
        "    (df_results[bart_ent] > CONSENSUS_ENTAIL_THRESHOLD) &\n",
        "    (df_results[rob_ent]  > CONSENSUS_ENTAIL_THRESHOLD)\n",
        ")\n",
        "\n",
        "\n",
        "# 7. Export with safe column selection\n",
        "desired_output_columns = [\n",
        "    PROPOSITION_COLUMN,\n",
        "    # NLI probabilities\n",
        "    \"bart-large-mnli_entailment\",\n",
        "    \"bart-large-mnli_neutral\",\n",
        "    \"bart-large-mnli_contradiction\",\n",
        "    \"roberta-large-mnli_entailment\",\n",
        "    \"roberta-large-mnli_neutral\",\n",
        "    \"roberta-large-mnli_contradiction\",\n",
        "    # Agency scores\n",
        "    \"bart-large-mnli_agency_score\",\n",
        "    \"roberta-large-mnli_agency_score\",\n",
        "    \"mean_agency_score\",\n",
        "    # Legacy diagnostic flag\n",
        "    \"consensus_entail_pos_flag\",\n",
        "    # Optional metadata if present in the JSONL\n",
        "    \"timestamp\",\n",
        "    \"sentiment_label\",\n",
        "    \"sentiment_score\",\n",
        "    \"sim_ai\",\n",
        "    \"lex\",\n",
        "    \"dep\",\n",
        "]\n",
        "\n",
        "existing_output_columns = [c for c in desired_output_columns if c in df_results.columns]\n",
        "df_output = df_results[existing_output_columns].copy()\n",
        "\n",
        "df_output.to_csv(OUTPUT_FILENAME, index=False)\n",
        "\n",
        "print(\"\\n--- MNLI Inference Pipeline Complete ---\")\n",
        "print(f\"Results saved to: {OUTPUT_FILENAME}\")\n",
        "print(\"\\n--- Preview (first 5 rows) ---\")\n",
        "print(df_output.head())\n",
        "print(\"-\" * 40)\n"
      ]
    }
  ]
}